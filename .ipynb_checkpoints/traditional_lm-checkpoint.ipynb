{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "935d6977-8c92-48ab-90b0-468014d15191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db0aaa88-111f-4146-a0fc-bacf75876e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'wiki-en-train.word'\n",
    "test_path = 'wiki-en-test.word'\n",
    "\n",
    "train_sents = []\n",
    "with open(train_path) as f:\n",
    "    for row in f.readlines():\n",
    "        train_sents.append(row.strip())\n",
    "\n",
    "test_sents = []\n",
    "with open(test_path) as f:\n",
    "    for row in f.readlines():\n",
    "        test_sents.append(row.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "455d0c0a-d4b1-4b10-85a5-6b642172c545",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLM:\n",
    "    def __init__(self, n):\n",
    "        assert isinstance(n, int), 'n must be integer'\n",
    "        assert n > 1, 'n must be larger than 1'\n",
    "        self.n = n\n",
    "        self.count_ngram = defaultdict(int)\n",
    "        self.count_next = defaultdict(lambda: defaultdict(int))\n",
    "        \n",
    "        self.num_ngram = 0\n",
    "        \n",
    "    def train(self, sents):\n",
    "        ## count frequency\n",
    "        for sent in sents:\n",
    "            \n",
    "            # simple tokenization\n",
    "            tokens = ['<s>'] * (self.n-1) + sent.split(' ') + ['</s>'] * (self.n-1)\n",
    "\n",
    "            for i in range(len(tokens)-self.n+1):\n",
    "                ngram = tuple(tokens[i:i+self.n])\n",
    "                h = ngram[:-1]  # history\n",
    "                w = ngram[-1]  # next word\n",
    "                \n",
    "                # count n-gram frequency\n",
    "                self.count_ngram[ngram] += 1\n",
    "\n",
    "                # count history and the next word frequency\n",
    "                self.count_next[h][w] += 1\n",
    "                \n",
    "        self.num_ngram = sum([len(self.count_next[key]) for key in self.count_next.keys()])\n",
    "        \n",
    "    def calc_prob_ngram(self, h, w, d=0.75):\n",
    "        h_next = len(self.count_next[h])\n",
    "        if h_next == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            # kneser-ney smoothing\n",
    "            h_total = sum(self.count_next[h].values())\n",
    "            lmb = d * h_next / h_total\n",
    "            \n",
    "            p_cont = len([key for key in self.count_next.keys() if w in self.count_next[key]]) / self.num_ngram\n",
    "            p_kn = max(self.count_ngram[(h + (w, ))]-d, 0) / h_next + lmb * p_cont\n",
    "            \n",
    "            return p_kn\n",
    "        \n",
    "    def calc_mle(self, sent):\n",
    "        # simple tokenization\n",
    "        tokens = tuple(['<s>'] * (self.n-1) + sent.split(' ') + ['</s>'] * (self.n-1))\n",
    "        \n",
    "        prob = 0.\n",
    "        for i in range(len(tokens)-self.n+1):            \n",
    "            p = self.calc_prob_ngram(tokens[i:i+self.n-1], tokens[(i+self.n-1)%(len(tokens))])\n",
    "            if p != 0:\n",
    "                prob += -np.log(p) / len(sent.split())\n",
    "\n",
    "        return prob\n",
    "    \n",
    "    def calc_mle_corpus(self, sents):\n",
    "        prob = 0.\n",
    "        for sent in sents:\n",
    "            prob += self.calc_mle(sent)\n",
    "\n",
    "        return prob / len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91e6ff22-9cb1-4248-a312-45c53705bc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n : 2, entropy : 3.7080974631812897\n",
      "n : 3, entropy : 1.8182439265814083\n",
      "n : 4, entropy : 0.428697616506698\n",
      "n : 5, entropy : 0.1205694109177372\n"
     ]
    }
   ],
   "source": [
    "for n in [2, 3, 4, 5]:\n",
    "    ng = NGramLM(n)\n",
    "    ng.train(train_sents)\n",
    "    print('n : {}, entropy : {}'.format(n, ng.calc_mle_corpus(test_sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4827ca-f8a3-4957-9818-1d9f418ddfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
